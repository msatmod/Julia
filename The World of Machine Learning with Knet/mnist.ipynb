{
 "cells": [
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Pkg; Pkg.add(Pkg.PackageSpec(url=\"https://github.com/JuliaComputing/JuliaAcademyData.jl\"))\n",
    "using JuliaAcademyData; activate(\"World of machine learning\")\n",
    "cd(datapath(\"mnist\"))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Load and minibatch MNIST data\n",
    "(c) Deniz Yuret, 2018"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "* Objective: Learning the structure of the MNIST dataset and usage of the Knet.Data struct.\n",
    "* Prerequisites: [The iteration interface](https://docs.julialang.org/en/v1/manual/interfaces/#man-interface-iteration-1)\n",
    "* Knet: mnistdata, mnistview (used and explained)\n",
    "* Knet: dir, minibatch (used by mnist.jl)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# This loads MNIST handwritten digit recognition dataset.\n",
    "# Traininig and test data go to variables dtrn and dtst\n",
    "using Knet: Knet, minibatch\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))  # defines mnistdata and mnistview\n",
    "dtrn,dtst = mnistdata(xtype=Array{Float32});"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# dtrn contains 600 minibatches of 100 images (total 60000)\n",
    "# dtst contains 100 minibatches of 100 images (total 10000)\n",
    "length.((dtrn,dtst))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Each minibatch is an (x,y) pair where x is 100 28x28x1 images and y contains their labels.\n",
    "# Here is the first minibatch in the test set:\n",
    "(x,y) = first(dtst)\n",
    "summary.((x,y))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Here is the first five images from the test set:\n",
    "using Images, ImageMagick\n",
    "hcat([mnistview(x,i) for i=1:5]...)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Here are their labels (0x0a=10 is used to represent 0)\n",
    "@show y[1:5];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# dtrn and dtst are implemented as Julia iterators (see https://docs.julialang.org/en/v1/manual/interfaces)\n",
    "# This means they can be used in for loops, i.e. `for (x,y) in dtst`\n",
    "cnt = zeros(Int,10)\n",
    "for (x,y) in dtst\n",
    "    for label in y\n",
    "        cnt[label] += 1\n",
    "    end\n",
    "end\n",
    "@show cnt;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Convolutional Neural Networks\n",
    "(c) Deniz Yuret, 2018"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "* Objectives: See the effect of sparse and shared weights implemented by convolutional networks.\n",
    "* Prerequisites: MLP models (04.mlp.ipynb), KnetArray, param, param0, dropout, relu, nll\n",
    "* Knet: conv4, pool, mat (explained)\n",
    "* Knet: dir, gpu, minibatch, KnetArray (used by mnist.jl)\n",
    "* Knet: SGD, train!, Train, load, save (used by trainresults)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Introduction to convolution"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Convolution operator in Knet\n",
    "using Knet: conv4\n",
    "@doc conv4"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Convolution in 1-D\n",
    "@show w = reshape([1.0,2.0,3.0], (3,1,1,1))\n",
    "@show x = reshape([1.0:7.0...], (7,1,1,1))\n",
    "@show y = conv4(w, x);  # size Y = X - W + 1 = 5 by default"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Padding\n",
    "@show y2 = conv4(w, x, padding=(1,0));  # size Y = X + 2P - W + 1 = 7 with padding=1\n",
    "# To preserve input size (Y=X) for a given W, what padding P should we use?"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Stride\n",
    "@show y3 = conv4(w, x; padding=(1,0), stride=3);  # size Y = 1 + floor((X+2P-W)/S)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Mode\n",
    "@show y4 = conv4(w, x, mode=0);  # Default mode (convolution) inverts w\n",
    "@show y5 = conv4(w, x, mode=1);  # mode=1 (cross-correlation) does not invert w"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Convolution in more dimensions\n",
    "x = reshape([1.0:9.0...], (3,3,1,1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "w = reshape([1.0:4.0...], (2,2,1,1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y = conv4(w, x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Convolution with multiple channels, filters, and instances\n",
    "# size X = [X1,X2,...,Xd,Cx,N] where d is the number of dimensions, Cx is channels, N is instances\n",
    "x = reshape([1.0:18.0...], (3,3,2,1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# size W = [W1,W2,...,Wd,Cx,Cy] where d is the number of dimensions, Cx is input channels, Cy is output channels\n",
    "w = reshape([1.0:24.0...], (2,2,2,3));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# size Y = [Y1,Y2,...,Yd,Cy,N]  where Yi = 1 + floor((Xi+2Pi-Wi)/Si), Cy is channels, N is instances\n",
    "y = conv4(w,x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "See http://cs231n.github.io/assets/conv-demo/index.html for an animated example."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Introduction to Pooling"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Pooling operator in Knet\n",
    "using Knet: pool\n",
    "@doc pool"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# 1-D pooling example\n",
    "@show x = reshape([1.0:6.0...], (6,1,1,1))\n",
    "@show pool(x);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Window size\n",
    "@show pool(x; window=3);  # size Y = floor(X/W)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Padding\n",
    "@show pool(x; padding=(1,0));  # size Y = floor((X+2P)/W)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Stride\n",
    "@show x = reshape([1.0:10.0...], (10,1,1,1));\n",
    "@show pool(x; stride=4);  # size Y = 1 + floor((X+2P-W)/S)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Mode (using Array here; not all modes are implemented on the CPU)\n",
    "using Knet: KnetArray\n",
    "x = Array(reshape([1.0:6.0...], (6,1,1,1)))\n",
    "@show x\n",
    "@show pool(x; padding=(1,0), mode=0)  # max pooling\n",
    "@show pool(x; padding=(1,0), mode=1)  # avg pooling\n",
    "# @show pool(x; padding=(1,0), mode=2); # avg pooling excluding padded values (is not implemented on CPU so will error)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# More dimensions\n",
    "x = reshape([1.0:16.0...], (4,4,1,1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pool(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Multiple channels and instances\n",
    "x = reshape([1.0:32.0...], (4,4,2,1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# each channel and each instance is pooled separately\n",
    "pool(x)  # size Y = (Y1,...,Yd,Cx,N) where Yi are spatial dims, Cx and N are identical to input X"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Experiment setup"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Load data (see 02.mnist.ipynb)\n",
    "using Knet: Knet, KnetArray, gpu, minibatch\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))  # Load data\n",
    "dtrn,dtst = mnistdata();              # dtrn and dtst = [ (x1,y1), (x2,y2), ... ] where xi,yi are minibatches of 100"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "(x,y) = first(dtst)\n",
    "summary.((x,y))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# For running experiments\n",
    "using Knet: SGD, train!, nll, zeroone\n",
    "import ProgressMeter\n",
    "\n",
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \");readline()[1]=='y')\n",
    "        results = Float64[]; updates = 0; prog = ProgressMeter.Progress(60000)\n",
    "        function callback(J)\n",
    "            if updates % 600 == 0\n",
    "                push!(results, nll(model,dtrn), nll(model,dtst), zeroone(model,dtrn), zeroone(model,dtst))\n",
    "                ProgressMeter.update!(prog, updates)\n",
    "            end\n",
    "            return (updates += 1) <= 60000\n",
    "        end\n",
    "        train!(model, dtrn; callback=callback, optimizer=SGD(lr=0.1), o...)\n",
    "        Knet.save(file,\"results\",reshape(results, (4,:)))\n",
    "    end\n",
    "    isfile(file) || download(\"http://people.csail.mit.edu/deniz/models/tutorial/$file\",file)\n",
    "    results = Knet.load(file,\"results\")\n",
    "    println(minimum(results,dims=2))\n",
    "    return results\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## A convolutional neural network model for MNIST"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Redefine Linear layer (See 03.lin.ipynb):\n",
    "using Knet: param, param0\n",
    "struct Linear; w; b; end\n",
    "(f::Linear)(x) = (f.w * mat(x) .+ f.b)\n",
    "mat(x)=reshape(x,:,size(x)[end])  # Reshapes 4-D tensor to 2-D matrix so we can use matmul\n",
    "Linear(inputsize::Int,outputsize::Int) = Linear(param(outputsize,inputsize),param0(outputsize))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Define a convolutional layer:\n",
    "struct Conv; w; b; end\n",
    "(f::Conv)(x) = pool(conv4(f.w,x) .+ f.b)\n",
    "Conv(w1,w2,cx,cy) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Define a convolutional neural network:\n",
    "struct CNN; layers; end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Weight initialization for a multi-layer convolutional neural network\n",
    "# h[i] is an integer for a fully connected layer, a triple of integers for convolution filters and tensor inputs\n",
    "# use CNN(x,h1,h2,...,hn,y) for a n hidden layer model\n",
    "function CNN(h...)\n",
    "    w = Any[]\n",
    "    x = h[1]\n",
    "    for i=2:length(h)\n",
    "        if isa(h[i],Tuple)\n",
    "            (x1,x2,cx) = x\n",
    "            (w1,w2,cy) = h[i]\n",
    "            push!(w, Conv(w1,w2,cx,cy))\n",
    "            x = ((x1-w1+1)รท2,(x2-w2+1)รท2,cy) # assuming conv4 with p=0, s=1 and pool with p=0,w=s=2\n",
    "        elseif isa(h[i],Integer)\n",
    "            push!(w, Linear(prod(x),h[i]))\n",
    "            x = h[i]\n",
    "        else\n",
    "            error(\"Unknown layer type: $(h[i])\")\n",
    "        end\n",
    "    end\n",
    "    CNN(w)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Knet: dropout, relu\n",
    "function (m::CNN)(x; pdrop=0)\n",
    "    for (i,layer) in enumerate(m.layers)\n",
    "        p = (i <= length(pdrop) ? pdrop[i] : pdrop[end])\n",
    "        x = dropout(x, p)\n",
    "        x = layer(x)\n",
    "        x = (layer == m.layers[end] ? x : relu.(x))\n",
    "    end\n",
    "    return x\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lenet = CNN((28,28,1), (5,5,20), (5,5,50), 500, 10)\n",
    "summary.(l.w for l in lenet.layers)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Knet: nll\n",
    "(x,y) = first(dtst)\n",
    "nll(lenet,x,y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## CNN vs MLP"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Plots; default(fmt=:png,ls=:auto)\n",
    "ENV[\"COLUMNS\"] = 92"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@time cnn = trainresults(\"cnn.jld2\", lenet; pdrop=(0,0,.3)); # 406s [8.83583e-5, 0.017289, 0.0, 0.0048]\n",
    "# Note that training will take a very long time without a GPU"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isfile(\"mlp.jld2\") || download(\"http://people.csail.mit.edu/deniz/models/tutorial/mlp.jld2\",\"mlp.jld2\")\n",
    "mlp = Knet.load(\"mlp.jld2\",\"results\");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Comparison to MLP shows faster convergence, better generalization\n",
    "plot([mlp[1,:], mlp[2,:], cnn[1,:], cnn[2,:]],ylim=(0.0,0.1),\n",
    "     labels=[:trnMLP :tstMLP :trnCNN :tstCNN],xlabel=\"Epochs\",ylabel=\"Loss\")\n",
    "plot([mlp[3,:], mlp[4,:], cnn[3,:], cnn[4,:]],ylim=(0.0,0.03),\n",
    "    labels=[:trnMLP :tstMLP :trnCNN :tstCNN],xlabel=\"Epochs\",ylabel=\"Error\")\n",
    "# ## Convolution vs Matrix Multiplication\n",
    "# Convolution and matrix multiplication can be implemented in terms of each other.\n",
    "# Convolutional networks have no additional representational power, only statistical efficiency.\n",
    "# Our original 1-D example\n",
    "@show w = reshape([1.0,2.0,3.0], (3,1,1,1))\n",
    "@show x = reshape([1.0:7.0...], (7,1,1,1))\n",
    "@show y = conv4(w, x);  # size Y = X - W + 1 = 5 by default"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Convolution as matrix multiplication (1)\n",
    "# Turn w into a (Y,X) sparse matrix\n",
    "w2 = Float64[3 2 1 0 0 0 0; 0 3 2 1 0 0 0; 0 0 3 2 1 0 0; 0 0 0 3 2 1 0; 0 0 0 0 3 2 1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show y2 = w2 * mat(x);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Convolution as matrix multiplication (2)\n",
    "# Turn x into a (W,Y) dense matrix (aka the im2col operation)\n",
    "# This is used to speed up convolution with known efficient matmul algorithms\n",
    "x3 = Float64[1 2 3 4 5; 2 3 4 5 6; 3 4 5 6 7]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show w3 = [3.0 2.0 1.0]\n",
    "@show y3 = w3 * x3;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Matrix multiplication as convolution\n",
    "# This could be used to make a fully connected network accept variable sized inputs.\n",
    "w = reshape([1.0:6.0...], (2,3))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = reshape([1.0:3.0...], (3,1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y = w * x"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Consider w with size (Y,X)\n",
    "# Treat each of the Y rows of w as a convolution filter\n",
    "w2 = copy(reshape(Array(w)', (3,1,1,2)))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Reshape x for convolution\n",
    "x2 = reshape(x, (3,1,1,1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Use conv4 for matrix multiplication\n",
    "y2 = conv4(w2, x2; mode=1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# So there is no difference between the class of functions representable with an MLP vs CNN.\n",
    "# Sparse connections and weight sharing give CNNs more generalization power with images.\n",
    "# Number of parameters in MLP256: (256x784)+256+(10x256)+10 = 203530\n",
    "# Number of parameters in LeNet: (5*5*1*20)+20+(5*5*20*50)+50+(500*800)+500+(10*500)+10 = 431080"
   ],
   "metadata": {},
   "execution_count": null
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  },
  "kernelspec": {
   "name": "julia-1.0",
   "display_name": "Julia 1.0.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
